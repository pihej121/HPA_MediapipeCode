{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2t5HFgcYEEI",
        "outputId": "bae1e38c-c12b-404c-b742-3ad96c0e1b2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting mediapipe\n",
            "  Downloading mediapipe-0.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.9/33.9 MB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.7.0.72)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from mediapipe) (1.4.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (23.1.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (23.3.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from mediapipe) (3.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from mediapipe) (1.22.4)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.10/dist-packages (from mediapipe) (4.7.0.72)\n",
            "Requirement already satisfied: protobuf<4,>=3.11 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (3.20.3)\n",
            "Collecting sounddevice>=0.4.4 (from mediapipe)\n",
            "  Downloading sounddevice-0.4.6-py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.10/dist-packages (from sounddevice>=0.4.4->mediapipe) (1.15.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (4.39.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (8.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (2.8.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.21)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n",
            "Installing collected packages: sounddevice, mediapipe\n",
            "Successfully installed mediapipe-0.10.0 sounddevice-0.4.6\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.22.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.10.1)\n",
            "Requirement already satisfied: Sphinx in /usr/local/lib/python3.10/dist-packages (3.5.4)\n",
            "Collecting numpydoc\n",
            "  Downloading numpydoc-1.5.0-py3-none-any.whl (52 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.4/52.4 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nose\n",
            "  Downloading nose-1.3.7-py3-none-any.whl (154 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.7/154.7 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pykalman\n",
            "  Downloading pykalman-0.9.5.tar.gz (228 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m228.9/228.9 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: sphinxcontrib-applehelp in /usr/local/lib/python3.10/dist-packages (from Sphinx) (1.0.4)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp in /usr/local/lib/python3.10/dist-packages (from Sphinx) (1.0.2)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath in /usr/local/lib/python3.10/dist-packages (from Sphinx) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp in /usr/local/lib/python3.10/dist-packages (from Sphinx) (2.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml in /usr/local/lib/python3.10/dist-packages (from Sphinx) (1.1.5)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp in /usr/local/lib/python3.10/dist-packages (from Sphinx) (1.0.3)\n",
            "Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.10/dist-packages (from Sphinx) (3.1.2)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.10/dist-packages (from Sphinx) (2.14.0)\n",
            "Requirement already satisfied: docutils<0.17,>=0.12 in /usr/local/lib/python3.10/dist-packages (from Sphinx) (0.16)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.10/dist-packages (from Sphinx) (2.2.0)\n",
            "Requirement already satisfied: babel>=1.3 in /usr/local/lib/python3.10/dist-packages (from Sphinx) (2.12.1)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.10/dist-packages (from Sphinx) (0.7.13)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.10/dist-packages (from Sphinx) (1.4.1)\n",
            "Requirement already satisfied: requests>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from Sphinx) (2.27.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from Sphinx) (67.7.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from Sphinx) (23.1)\n",
            "Collecting Sphinx\n",
            "  Downloading sphinx-7.0.0-py3-none-any.whl (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docutils<0.20,>=0.18.1 (from Sphinx)\n",
            "  Downloading docutils-0.19-py3-none-any.whl (570 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m570.5/570.5 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=2.3->Sphinx) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->Sphinx) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->Sphinx) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->Sphinx) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->Sphinx) (3.4)\n",
            "Building wheels for collected packages: pykalman\n",
            "  Building wheel for pykalman (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pykalman: filename=pykalman-0.9.5-py3-none-any.whl size=48442 sha256=0af8271914e5ed290f2a6f227e976a07dbf39b734327b95d95ab644ddf5d6e8f\n",
            "  Stored in directory: /root/.cache/pip/wheels/76/33/ef/5f332226e13a5089c6dd4b01cc2bcb59491d18f955fa2d3807\n",
            "Successfully built pykalman\n",
            "Installing collected packages: pykalman, nose, docutils, Sphinx, numpydoc\n",
            "  Attempting uninstall: docutils\n",
            "    Found existing installation: docutils 0.16\n",
            "    Uninstalling docutils-0.16:\n",
            "      Successfully uninstalled docutils-0.16\n",
            "  Attempting uninstall: Sphinx\n",
            "    Found existing installation: Sphinx 3.5.4\n",
            "    Uninstalling Sphinx-3.5.4:\n",
            "      Successfully uninstalled Sphinx-3.5.4\n",
            "Successfully installed Sphinx-7.0.0 docutils-0.19 nose-1.3.7 numpydoc-1.5.0 pykalman-0.9.5\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting keyboard\n",
            "  Downloading keyboard-0.13.5-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.1/58.1 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: keyboard\n",
            "Successfully installed keyboard-0.13.5\n"
          ]
        }
      ],
      "source": [
        "#run these commands when you start the program\n",
        "get_ipython().system('pip install mediapipe opencv-python')\n",
        "get_ipython().system('pip install numpy scipy Sphinx numpydoc nose pykalman')\n",
        "get_ipython().system('pip install keyboard')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rq_tEYZsWQ2W",
        "outputId": "5a6760b8-c338-4761-e6c9-bc4e3e24ce44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "55ogElZkedJ5"
      },
      "outputs": [],
      "source": [
        "#program imports\n",
        "import mediapipe as mp\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "import imageio\n",
        "import keyboard\n",
        "%matplotlib notebook\n",
        "import matplotlib.pyplot as plt\n",
        "from pykalman import KalmanFilter\n",
        "from google.colab.patches import cv2_imshow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pzYPGyHBfFYE"
      },
      "outputs": [],
      "source": [
        "# Rename some packages for ease of use\n",
        "mp_drawing = mp.solutions.drawing_utils\n",
        "mp_pose = mp.solutions.pose\n",
        "mp_hands = mp.solutions.hands"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CY0wOjKhfKc4"
      },
      "outputs": [],
      "source": [
        "# Name of input video file and extension\n",
        "name = \"HANDYA034_Center_Pinch_RH_1\"\n",
        "extension = \".mpeg\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MjbK6zTKf6Tj"
      },
      "outputs": [],
      "source": [
        "# Define some variables\n",
        "input_video_path    = ''\n",
        "output_video_path   = ''\n",
        "output_csv_path     = ''\n",
        "output_smooth_path  = ''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Lls0poEf9R5",
        "outputId": "d82292b2-6c93-4708-fb69-c2ef3d934d32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path of input video:  /content/drive/MyDrive/Mediapipe_Folders/input_videos/HandYA034/HANDYA034_Center_Pinch_RH_1.mpeg\n",
            "------------------------------------------\n",
            "Input video exists, continue with program!\n",
            "------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# if paths weren't explicitly stated, paths are assumed\n",
        "if input_video_path == \"\":\n",
        "    input_video_path = f'/content/drive/MyDrive/Mediapipe_Folders/input_videos/HandYA034/{name}{extension}'\n",
        "if output_video_path == \"\":\n",
        "    output_video_path = f'/content/drive/MyDrive/Mediapipe_Folders/output_videos/HandYA034_Output/{name}.avi'\n",
        "if output_csv_path == \"\":\n",
        "    output_csv_path = f'/content/drive/MyDrive/Mediapipe_Folders/csv/{name}.csv'\n",
        "if output_smooth_path == \"\":\n",
        "    output_smooth_path = f'/content/drive/MyDrive/Mediapipe_Folders/smooth_csv/{name}_smooth.csv'\n",
        "\n",
        "print(\"Path of input video: \",input_video_path)\n",
        "print(\"------------------------------------------\")\n",
        "if (os.path.exists(input_video_path)):\n",
        "    print(\"Input video exists, continue with program!\")\n",
        "    print(\"------------------------------------------\")\n",
        "else:\n",
        "    print(\"file not found, please correct the path!\")\n",
        "    print(\"------------------------------------------\")\n",
        "if (os.path.exists(output_video_path)):\n",
        "    print(\"There already exists a video of this name\")\n",
        "    print(\"Are you sure you want to overwrite it?\")\n",
        "    print(\"------------------------------------------\")\n",
        "\n",
        "if (os.path.exists(output_csv_path)):\n",
        "    print(\"There already exists a csv of this name\")\n",
        "    print(\"Are you sure you want to overwrite it?\")\n",
        "    print(\"------------------------------------------\")\n",
        "\n",
        "# Pay attention to output below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xGQ9RzUiR88",
        "outputId": "132bb8f3-d3e2-4b2b-c804-48ab546f3a9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Video is: 1920 by 1080 at 59 FPS\n"
          ]
        }
      ],
      "source": [
        "# open video in opencv and initialize output video\n",
        "# cap = cv2.VideoCapture(0) #webcam for testing\n",
        "cap = cv2.VideoCapture(input_video_path)\n",
        "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "fps_input = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "fourcc = cv2.VideoWriter_fourcc(*'XVID')  # you can adjust codec depending on os, preference, etc.\n",
        "print(\"Input Video is: \" + str(frame_width) + \" by \" + str(frame_height) + \" at \" + str(fps_input) + \" FPS\")\n",
        "out = cv2.VideoWriter(output_video_path, fourcc, fps_input, (frame_width, frame_height))\n",
        "# might give you some warnings about a codec not supported\n",
        "# not very important, but it may be difficult for others to play the video with VLC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJNHv3kNihNU",
        "outputId": "0bd1ca30-15ec-47d2-dceb-3df49c918b57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of points: 48\n"
          ]
        }
      ],
      "source": [
        "# define landmarks and csv columns\n",
        "landmark_names_face = ['NOSE', 'LEFT_EYE_IN', 'LEFT_EYE', 'LEFT_EYE_OUT', 'RIGHT_EYE_IN', 'RIGHT_EYE', 'RIGHT_EYE_OUT',\n",
        "                       'LEFT_EAR', 'RIGHT_EAR', 'MOUTH_LEFT', 'MOUTH_RIGHT']\n",
        "landmark_names_body = ['LEFT_SHOULDER', 'RIGHT_SHOULDER', 'LEFT_ELBOW', 'RIGHT_ELBOW', 'LEFT_WRIST', 'RIGHT_WRIST',\n",
        "                       'LEFT_PINKY', 'RIGHT_PINKY', 'LEFT_INDEX', 'RIGHT_INDEX', 'LEFT_THUMB', 'RIGHT_THUMB',\n",
        "                       'LEFT_HIP', 'RIGHT_HIP', 'LEFT_KNEE', 'RIGHT_KNEE', 'LEFT_ANKLE', 'RIGHT_ANKLE',\n",
        "                       'LEFT_HEEL', 'RIGHT_HEEL', 'LEFT_FOOT_INDEX', 'RIGHT_FOOT_INDEX']\n",
        "landmark_names_pose = np.concatenate([landmark_names_face, landmark_names_body])\n",
        "\n",
        "POSE_LANDMARK_INDICES = [11, 12, 13, 14, 15, 16]\n",
        "POSE_LANDMARK_NAMES = [landmark_names_pose[i] for i in POSE_LANDMARK_INDICES]\n",
        "\n",
        "# Generate landmark names\n",
        "landmark_names_pose = POSE_LANDMARK_NAMES\n",
        "landmark_names_left = ['LEFT_WRIST_HAND', 'LEFT_THUMB_1', 'LEFT_THUMB_2', 'LEFT_THUMB_3', 'LEFT_THUMB_4',\n",
        "                       'LEFT_INDEX_1', 'LEFT_INDEX_2', 'LEFT_INDEX_3', 'LEFT_INDEX_4',\n",
        "                       'LEFT_MIDDLE_1', 'LEFT_MIDDLE_2', 'LEFT_MIDDLE_3', 'LEFT_MIDDLE_4',\n",
        "                       'LEFT_RING_1', 'LEFT_RING_2', 'LEFT_RING_3', 'LEFT_RING_4',\n",
        "                       'LEFT_PINKY_1', 'LEFT_PINKY_2', 'LEFT_PINKY_3', 'LEFT_PINKY_4']\n",
        "landmark_names_right = ['RIGHT_WRIST_HAND', 'RIGHT_THUMB_1', 'RIGHT_THUMB_2', 'RIGHT_THUMB_3', 'RIGHT_THUMB_4',\n",
        "                        'RIGHT_INDEX_1', 'RIGHT_INDEX_2', 'RIGHT_INDEX_3', 'RIGHT_INDEX_4',\n",
        "                        'RIGHT_MIDDLE_1', 'RIGHT_MIDDLE_2', 'RIGHT_MIDDLE_3', 'RIGHT_MIDDLE_4',\n",
        "                        'RIGHT_RING_1', 'RIGHT_RING_2', 'RIGHT_RING_3', 'RIGHT_RING_4',\n",
        "                        'RIGHT_PINKY_1', 'RIGHT_PINKY_2', 'RIGHT_PINKY_3', 'RIGHT_PINKY_4']\n",
        "landmark_names_all = np.hstack([landmark_names_pose, landmark_names_left, landmark_names_right])\n",
        "\n",
        "# Adds in the _X _Y _Z to all the landmark names\n",
        "landmark_names_pose = [name + suffix\n",
        "                       for name in landmark_names_pose\n",
        "                       for suffix in ['_X', '_Y', '_Z']]\n",
        "landmark_names_left = [name + suffix\n",
        "                       for name in landmark_names_left\n",
        "                       for suffix in ['_X', '_Y', '_Z']]\n",
        "landmark_names_right = [name + suffix\n",
        "                        for name in landmark_names_right\n",
        "                        for suffix in ['_X', '_Y', '_Z']]\n",
        "landmark_names_all = [name + suffix\n",
        "                      for name in landmark_names_all\n",
        "                      for suffix in ['_X', '_Y', '_Z']]\n",
        "\n",
        "len_landmarks_all = len(landmark_names_all)\n",
        "landmark_names_all = np.array(landmark_names_all)\n",
        "landmark_names_all.shape = (1, len_landmarks_all)\n",
        "\n",
        "len_landmarks_all = int(len_landmarks_all / 3)\n",
        "\n",
        "print(\"number of points:\", len_landmarks_all)\n",
        "\n",
        "# initializing data arrays\n",
        "data_left_len = int(len(landmark_names_left) / 3) # Should be 21\n",
        "data_right_len = int(len(landmark_names_right) / 3) # Should be 21\n",
        "data_pose_len = int(len(landmark_names_pose) / 3) # Should be  6\n",
        "\n",
        "data_left = np.zeros(shape=(1, data_left_len, 3))  # Left Hand Data\n",
        "data_right = np.zeros(shape=(1, data_right_len, 3))  # Right Hand Data\n",
        "data_pose = np.zeros(shape=(1, data_pose_len, 3))  # Pose Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RWSS3O5TilmY"
      },
      "outputs": [],
      "source": [
        "# Rename more packages and some drawing connections\n",
        "POSE_CONNECTIONS = mp.solutions.pose.POSE_CONNECTIONS\n",
        "HAND_CONNECTIONS = mp.solutions.hands_connections.HAND_CONNECTIONS\n",
        "\n",
        "VALID_POSE_CONNECTIONS = []\n",
        "for a, b in POSE_CONNECTIONS:\n",
        "    if a in POSE_LANDMARK_INDICES and b in POSE_LANDMARK_INDICES:\n",
        "        VALID_POSE_CONNECTIONS.append((a, b))\n",
        "POSE_CONNECTIONS = VALID_POSE_CONNECTIONS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9IBv7xlNjEJ6"
      },
      "outputs": [],
      "source": [
        "# This function will run the Kalman Smoothing we will use later\n",
        "# Some parameters can be changed to make the smoothing more similar to what is desired\n",
        "def smooth(arr, initial_state_mean=0, b=-1):\n",
        "    intermed = arr.swapaxes(0, 2)\n",
        "    iter_total = intermed.shape[0] * intermed.shape[1]\n",
        "    iter = 1\n",
        "    for i in range(intermed.shape[0]):\n",
        "        for j in range(intermed.shape[1]):\n",
        "            kf = KalmanFilter(initial_state_mean=initial_state_mean, n_dim_obs=1)\n",
        "            measurements = np.ma.masked_invalid(intermed[i][j])\n",
        "            if b > -1:\n",
        "                kf = kf.em(measurements, n_iter=b)\n",
        "            else:\n",
        "                kf = kf.em(measurements)\n",
        "            smooth_meas = kf.smooth(measurements)[0]\n",
        "            intermed[i][j] = smooth_meas.T[0]\n",
        "            sys.stdout.write(f'\\rFinished smoothing point {iter} of {iter_total}')\n",
        "            sys.stdout.flush()\n",
        "            iter += 1\n",
        "\n",
        "    return intermed.swapaxes(0, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-VFSGDLljKu7",
        "outputId": "1886f8ce-6847-42e0-e671-1c4bcc7e7013"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading model to /usr/local/lib/python3.10/dist-packages/mediapipe/modules/pose_landmark/pose_landmark_heavy.tflite\n",
            "no detection pose\n",
            "Tracking complete, 1430 frames analyzed\n",
            "Percent Missed detections: 0.07%\n",
            "Your tracked output video will be saved to /content/drive/MyDrive/Mediapipe_Folders/output_videos/HandYA034_Output/HANDYA034_Center_Pinch_RH_1.avi\n",
            "Your output .csv data file will be saved to /content/drive/MyDrive/Mediapipe_Folders/csv/HANDYA034_Center_Pinch_RH_1.csv\n",
            "Your smoothed output .csv data file will be saved to /content/drive/MyDrive/Mediapipe_Folders/smooth_csv/HANDYA034_Center_Pinch_RH_1_smooth.csv\n"
          ]
        }
      ],
      "source": [
        "# Main mediapipe program block\n",
        "iter = 0\n",
        "missed = 0\n",
        "BREAK = False\n",
        "\n",
        "# may give some warnings about \"QObject\" but thats because pyqt5 and opencv do not get along\n",
        "# just ignore it, there is not much we can do to fix it nor is it really a problem\n",
        "with mp_pose.Pose(\n",
        "        min_detection_confidence=0.6,\n",
        "        min_tracking_confidence=0.6,\n",
        "        model_complexity=2) as pose:  # calibrate detection and tracking confidence for best results\n",
        "    with mp_hands.Hands(min_detection_confidence=0.6,\n",
        "                        min_tracking_confidence=0.6,\n",
        "                        max_num_hands=2) as hands:\n",
        "\n",
        "        while cap.isOpened():\n",
        "            success, image = cap.read()\n",
        "            if not success:\n",
        "                print(\"Tracking complete, \" + str(iter) + ' frames analyzed')\n",
        "                print(f\"Percent Missed detections: %.2f%%\" % ((missed / iter) * 100))\n",
        "                print(\"Your tracked output video will be saved to \" + output_video_path)\n",
        "                print(\"Your output .csv data file will be saved to \" + output_csv_path)\n",
        "                print(\"Your smoothed output .csv data file will be saved to \" + output_smooth_path)\n",
        "                break\n",
        "\n",
        "            # To improve performance, optionally mark the image as not writeable to\n",
        "            # pass by reference.\n",
        "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "            # image = cv2.flip(image, 1)\n",
        "\n",
        "            image.flags.writeable = False\n",
        "\n",
        "            results_pose = pose.process(image)\n",
        "            results_hand = hands.process(image)\n",
        "\n",
        "            # Draw landmark annotation on the image.\n",
        "            image.flags.writeable = True\n",
        "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "            POSE_DETECTED = results_pose.pose_world_landmarks\n",
        "\n",
        "            L_HAND = (False, -1)\n",
        "            R_HAND = (False, -1)\n",
        "            if results_hand.multi_hand_world_landmarks:\n",
        "                num_hands = len(results_hand.multi_hand_landmarks)\n",
        "                for hand_landmarks in results_hand.multi_hand_landmarks:\n",
        "                    mp_drawing.draw_landmarks(image, hand_landmarks, HAND_CONNECTIONS)\n",
        "                for i in range(num_hands):\n",
        "                    handedness = results_hand.multi_handedness[i]\n",
        "                    if handedness.classification[0].label == \"Left\":  # Right hand\n",
        "                        R_HAND = (True, i)\n",
        "                    if handedness.classification[0].label == \"Right\":  # Left hand\n",
        "                        L_HAND = (True, i)\n",
        "\n",
        "            POSE_WRIST_L = 15\n",
        "            POSE_WRIST_R = 16\n",
        "\n",
        "            # If left hand is detected\n",
        "            if L_HAND[0]:\n",
        "                data_temp = [[nl.x, nl.y, nl.z]\n",
        "                             for nl in results_hand.multi_hand_world_landmarks[L_HAND[1]].landmark]\n",
        "                data_left = np.vstack([data_left, [data_temp]])\n",
        "            else:\n",
        "\n",
        "                # Fill Left Hand array with NaN if not detected\n",
        "                data_nan = np.empty(shape=(1, data_left_len, 3))\n",
        "                data_nan[:] = np.NaN\n",
        "                data_left = np.vstack([data_left, data_nan])\n",
        "\n",
        "            # If right hand detected\n",
        "            if R_HAND[0]:\n",
        "                data_temp = [[nl.x, nl.y, nl.z]\n",
        "                             for nl in results_hand.multi_hand_world_landmarks[R_HAND[1]].landmark]\n",
        "\n",
        "                data_right = np.vstack([data_right, [data_temp]])\n",
        "            else:\n",
        "\n",
        "                # Fill Right Hand array with NaN if not detected\n",
        "                data_nan = np.empty(shape=(1, data_right_len, 3))\n",
        "                data_nan[:] = np.NaN\n",
        "                data_right = np.vstack([data_right, data_nan])\n",
        "\n",
        "            if (POSE_DETECTED):\n",
        "                wanted_landmarks = [results_pose.pose_world_landmarks.landmark[i] for i in POSE_LANDMARK_INDICES]\n",
        "                data_temp = [[nl.x, nl.y, nl.z]\n",
        "                             for nl in wanted_landmarks]\n",
        "                # iterate and save the landmarks to array\n",
        "                data_pose = np.vstack([data_pose, [data_temp]])\n",
        "\n",
        "                # Prioritize Hand's Wrist point over Pose's Wrist since Hand is more accurate\n",
        "                # Modify Pose points so that we can use MP's built-in utility to draw points\n",
        "                # We are still saving Pose's Wrist data point, but we would rather draw the Hand's wrist data point\n",
        "                m_pose_landmarks = results_pose.pose_landmarks\n",
        "\n",
        "                # Hide unwanted landmarks\n",
        "                for i in range(len(m_pose_landmarks.landmark)):\n",
        "                    if i not in POSE_LANDMARK_INDICES:\n",
        "                        m_pose_landmarks.landmark[i].visibility = 0\n",
        "\n",
        "                if L_HAND[0]:\n",
        "                    left_hand_wrist = results_hand.multi_hand_landmarks[L_HAND[1]].landmark[0]\n",
        "                    left_pose_wrist = m_pose_landmarks.landmark[POSE_WRIST_L]\n",
        "\n",
        "                    left_pose_wrist.x = left_hand_wrist.x\n",
        "                    left_pose_wrist.y = left_hand_wrist.y\n",
        "                    left_pose_wrist.z = left_hand_wrist.z\n",
        "                if R_HAND[0]:\n",
        "                    right_hand_wrist = results_hand.multi_hand_landmarks[R_HAND[1]].landmark[0]\n",
        "                    right_pose_wrist = m_pose_landmarks.landmark[POSE_WRIST_R]\n",
        "\n",
        "                    right_pose_wrist.x = right_hand_wrist.x\n",
        "                    right_pose_wrist.y = right_hand_wrist.y\n",
        "                    right_pose_wrist.z = right_hand_wrist.z\n",
        "\n",
        "\n",
        "                mp_drawing.draw_landmarks(\n",
        "                    image,\n",
        "                    m_pose_landmarks,\n",
        "                    POSE_CONNECTIONS)\n",
        "\n",
        "            else:\n",
        "                print(\"no detection pose\")\n",
        "                missed = missed + 1  # Missed variable only updates when pose is not detected (change?)\n",
        "\n",
        "                # Fill pose array with NaN if not detected\n",
        "                data_nan = np.empty(shape=(1, data_pose_len, 3))\n",
        "                data_nan[:] = np.NaN\n",
        "                data_pose = np.vstack([data_pose, data_nan])\n",
        "\n",
        "            iter += 1\n",
        "            out.write(image)\n",
        "            #cv2_imshow(image)  # must flip image\n",
        "\n",
        "            if cv2.waitKey(1) == ord('q'):  # press the \"q\" key to stop the tracking\n",
        "                # Will cause the csv to not output and numpy will throw\n",
        "                # an error on the next block if you stopped the video\n",
        "                BREAK = True\n",
        "                break;\n",
        "\n",
        "cap.release()\n",
        "out.release()\n",
        "cv2.destroyAllWindows()\n",
        "\n",
        "if BREAK:\n",
        "    exit(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kguzR7ABkjSq",
        "outputId": "3ea7014a-eb00-453e-8455-e449c5e78afe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Saving data to /content/drive/MyDrive/Mediapipe_Folders/csv/HANDYA034_Center_Pinch_RH_1.csv\n"
          ]
        }
      ],
      "source": [
        "packaged_data = np.hstack([data_pose, data_left, data_right])\n",
        "packaged_data = np.delete(packaged_data, 0, axis=0)  # Remove first \"row\" of placeholder data\n",
        "\n",
        "packaged_data.shape = (-1, packaged_data.shape[1] * packaged_data.shape[2])\n",
        "\n",
        "# save\n",
        "print(f\"\\nSaving data to {output_csv_path}\")\n",
        "with open(output_csv_path, 'a', newline='') as csvfile:  # output csv\n",
        "    np.savetxt(output_csv_path, landmark_names_all, fmt='%s', delimiter=',')\n",
        "    np.savetxt(csvfile, packaged_data, fmt='%f', delimiter=',')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P4uB7ivd7nP0"
      },
      "outputs": [],
      "source": [
        "# # Define some paths for the visualizer related to where we saved the previous files\n",
        "# input_csv_path = 'drive/MyDrive/colab_data/smooth_csv/dext_front_smooth.csv' \n",
        "# input_avi_path = 'drive/MyDrive/colab_data/output_videos/dext_front.avi'\n",
        "# FPS = 30 #you can adjust this to make the visualizer faster\n",
        "\n",
        "# if not os.path.exists(input_avi_path):\n",
        "#     print(\"Input video not found\")\n",
        "#     exit()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bkZKetLQm2wv"
      },
      "outputs": [],
      "source": [
        "# # Take in the mediapipe video to have a reference for the visualizer\n",
        "# cap = cv2.VideoCapture(input_avi_path)\n",
        "# frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "# frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "# fps_input = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "# fourcc = cv2.VideoWriter_fourcc(*'XVID')  # you can adjust codec depending on os, preference, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLfJiWKxm3eP"
      },
      "outputs": [],
      "source": [
        "# # NUMBER OF LANDMARKS\n",
        "# L_NUM, R_NUM, P_NUM = 21, 21, 6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a8XwbceEm8Pv"
      },
      "outputs": [],
      "source": [
        "# # Connections between hands\n",
        "# HAND_CONNECTIONS = mp.solutions.hands_connections.HAND_CONNECTIONS\n",
        "# POSE_CONNECTIONS = mp.solutions.pose.POSE_CONNECTIONS\n",
        "\n",
        "# POSE_INDICES = [11, 12, 13, 14, 15, 16]     # These are the indices of the points captured\n",
        "# pose_dict = dict(zip(POSE_INDICES, range(len(POSE_INDICES))))   # Convert indices to indices of pose array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XakYmihYnLDA"
      },
      "outputs": [],
      "source": [
        "# # Map the visible connections to indices of the pose_data array\n",
        "# VALID_CONNECTIONS = []\n",
        "# for a, b in POSE_CONNECTIONS:\n",
        "#     if a in POSE_INDICES and b in POSE_INDICES:\n",
        "#         VALID_CONNECTIONS.append((pose_dict[a], pose_dict[b]))\n",
        "\n",
        "# POSE_CONNECTIONS = VALID_CONNECTIONS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jw8_WSe2nN2j"
      },
      "outputs": [],
      "source": [
        "# # Data is formatted as (Pose, Left hand, Right hand)\n",
        "# with open(input_csv_path, 'r') as f:\n",
        "#     data = np.genfromtxt(f, delimiter=',', skip_header=1, dtype=float)\n",
        "#     data.shape = (-1, int(data.shape[1] / 3), 3)\n",
        "#     frames = data.shape[0]\n",
        "\n",
        "# split_indices = [P_NUM, L_NUM + P_NUM]       # Split array back into sub parts\n",
        "# pose_data, left_data, right_data = np.hsplit(data, split_indices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ciwP6Zm5r92r"
      },
      "outputs": [],
      "source": [
        "# # This block created the visualizer\n",
        "# # This takes a very long time close to 30 min occasionally\n",
        "# # It is very inefficient, but that is because of notebook style environments\n",
        "# # Loop initializations\n",
        "# do_save = True\n",
        "# figures = []\n",
        "\n",
        "# cap = cv2.VideoCapture(input_avi_path)\n",
        "\n",
        "# for frame in range(frames):\n",
        "#     # Set new points and line locations\n",
        "#     if cap.isOpened():\n",
        "#         success, image = cap.read()\n",
        "#         #cv2_imshow(image)\n",
        "#     # Init plot, probably not necessary to do this repeatly\n",
        "#     # But, it does not work without it\n",
        "#     plt.ion()\n",
        "#     fig = plt.figure(figsize=(50,50))\n",
        "#     ax = fig.add_subplot(111, projection='3d')\n",
        "#     ax.set_xlabel(\"x\")\n",
        "#     ax.set_ylabel(\"y\")\n",
        "#     ax.set_zlabel(\"z\")\n",
        "#     ax.set_xlim(-.5, .5)\n",
        "#     ax.set_ylim(-.5, .5)\n",
        "#     ax.set_zlim(-.5, .5)\n",
        "#     ax.set_title(\"Hand markers with z relative to wrist\")\n",
        "#     ax.view_init(elev=210, azim=90)\n",
        "#     # Change elev and azim to change the viewing angle of the 3-D plot\n",
        "#     # May be necessary for different videos\n",
        "#     temp = []\n",
        "#     temp.extend(HAND_CONNECTIONS)\n",
        "#     temp.extend(HAND_CONNECTIONS)\n",
        "#     temp.extend(POSE_CONNECTIONS)\n",
        "\n",
        "#     lines = [ax.plot([], [], [], color = \"green\")[0]\n",
        "#          for line in temp]\n",
        "#     scatters = ax.scatter([], [], [], color = \"red\")\n",
        "\n",
        "#     i = 0\n",
        "#     current_pose = pose_data[frame]\n",
        "#     current_left = left_data[frame]\n",
        "#     current_right = right_data[frame]\n",
        "#     L = current_left[0]\n",
        "#     R = current_right[0]\n",
        "#     current_left[:] += current_pose[4] - L\n",
        "#     current_right[:] += current_pose[5] - R\n",
        "#     for a, b in POSE_CONNECTIONS:\n",
        "#         lines[i].set_data_3d([current_pose[a][2], current_pose[b][2]],\n",
        "#                              [current_pose[a][0], current_pose[b][0]],\n",
        "#                              [current_pose[a][1], current_pose[b][1]])\n",
        "#         i += 1\n",
        "\n",
        "#     for a, b in HAND_CONNECTIONS:\n",
        "#         lines[i].set_data_3d([current_left[a][2], current_left[b][2]],\n",
        "#                              [current_left[a][0], current_left[b][0]],\n",
        "#                              [current_left[a][1], current_left[b][1]])\n",
        "#         i += 1\n",
        "#         lines[i].set_data_3d([current_right[a][2], current_right[b][2]],\n",
        "#                              [current_right[a][0], current_right[b][0]],\n",
        "#                              [current_right[a][1], current_right[b][1]])\n",
        "#         i += 1\n",
        "#     ditto = np.vstack([current_pose, current_left, current_right])\n",
        "#     scatters._offsets3d = (ditto.T[2], ditto.T[0], ditto.T[1])\n",
        "\n",
        "#     # Update graph\n",
        "#     plt.draw()\n",
        "#     fig.canvas.flush_events()\n",
        "#     filename = f\"{frame}.png\"\n",
        "#     if frame == 0:\n",
        "#         cv2.waitKey(1000000)\n",
        "#     plt.savefig(filename)\n",
        "#     figures.append(filename)\n",
        "#     plt.pause(1 / FPS)\n",
        "#     plt.close(fig)\n",
        "# if do_save:\n",
        "#     name = 'drive/MyDrive/colab_data/visualizer_videos/' + name + 'smooth_viz.mp4' \n",
        "#     # Will save file as an mp4 to the drive\n",
        "#     with imageio.get_writer(name, mode='I') as writer:\n",
        "#         for filename in figures:\n",
        "#             image = imageio.imread(filename)\n",
        "#             writer.append_data(image)\n",
        "# os.system(\"rm *.png\") # Removes from temporary static images we created"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}